{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bixbyone/ORRE/blob/main/orre1_0_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFZXqF4Aj2iG",
        "outputId": "a0e5b259-96f7-4130-f3de-cb3c91418625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n"
          ]
        }
      ],
      "source": [
        "pip install networkx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaqDoigjj-y7"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import csv\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSDNP1rnkTQS"
      },
      "outputs": [],
      "source": [
        "# G = nx.Graph()\n",
        "# G = nx.read_gml('western_interconnection.gml', label=None)\n",
        "# _G = G.copy()\n",
        "# def normalize(v):\n",
        "\n",
        "    # min, max = np.min(v), np.max(v)\n",
        "    # return((v - min)/(max - min))\n",
        "\n",
        "\n",
        "url = \"https://github.com/bixbyone/ORRE/blob/main/src/DataRecords.csv\" \n",
        "response = requests.get(url)\n",
        "rows = csv.DictReader(response.text.split('\\n'))\n",
        "# G = nx.readwrite.json_graph.node_link_graph()\n",
        "G = nx.read_csv(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPpGb5HkkiFN"
      },
      "outputs": [],
      "source": [
        "# URL do conjunto de dados remoto\n",
        "url = \"https://raw.githubusercontent.com/bixbyone/ORRE/main/src/DataRecords.json\"\n",
        "\n",
        "# Faz a requisição HTTP para obter o conjunto de dados\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida\n",
        "if response.status_code == 200:\n",
        "    # Carrega o JSON da resposta em um DataFrame do Pandas\n",
        "    df = pd.read_json(response.text)\n",
        "else:\n",
        "    # Trata o erro caso a requisição não seja bem-sucedida\n",
        "    print(\"Erro ao obter o conjunto de dados remoto:\", response.status_code)\n",
        "    exit()\n",
        "\n",
        "# Trata o DataFrame para normalizar os valores\n",
        "df[\"STR_DTPREVISTA\"] = pd.to_datetime(df[\"STR_DTPREVISTA\"], errors=\"coerce\")\n",
        "df[\"STR_DTENTRADA\"] = pd.to_datetime(df[\"STR_DTENTRADA\"], errors=\"coerce\")\n",
        "df[\"STR_DTPAR\"] = pd.to_datetime(df[\"STR_DTPAR\"], errors=\"coerce\")\n",
        "\n",
        "# Cria um dicionário a partir do DataFrame\n",
        "data = df.to_dict(orient=\"records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiiAgO2Awj17"
      },
      "outputs": [],
      "source": [
        "for node in sorted(degree, key=degree.get, reverse=True):\n",
        "  print(node, degree[node])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvuKSvMBygw_"
      },
      "outputs": [],
      "source": [
        "for node in sorted(eigenvector, key=eigenvector.get, reverse=True):\n",
        "  print(node, eigenvector[node])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZYg_Kcnkcsr"
      },
      "outputs": [],
      "source": [
        "for node in sorted(katz, key=katz.get, reverse=True):\n",
        "  print(node, katz[node])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw-ke-riyvsP"
      },
      "outputs": [],
      "source": [
        "for v in centrality_data.columns:\n",
        "    centrality_data[v] = normalize(centrality_data[v])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsQ-4g87ucZm"
      },
      "outputs": [],
      "source": [
        "centrality_correlations = centrality_data.corr('pearson')\n",
        "centrality_correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSwHoNKnvu5R"
      },
      "outputs": [],
      "source": [
        "sorted_degree = sorted(degree, key=degree.get, reverse=True)\n",
        "sorted_eigenvector = sorted(eigenvector, key=eigenvector.get, reverse=True)\n",
        "sorted_katz = sorted(katz, key=katz.get, reverse=True)\n",
        "\n",
        "def sample_central_nodes(G):\n",
        "  central_nodes = []\n",
        "  i = 0\n",
        "  for node in sorted(G, key=G.get, reverse=True):\n",
        "    if(i<100):\n",
        "      central_nodes.append(node)\n",
        "      i = i + 1\n",
        "  return central_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uZhqObc2CPn"
      },
      "outputs": [],
      "source": [
        "central_nodes_degree = sample_central_nodes(degree)\n",
        "central_nodes_eigenvector = sample_central_nodes(eigenvector)\n",
        "central_nodes_katz = sample_central_nodes(katz)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2syP9O8W2x-x"
      },
      "outputs": [],
      "source": [
        "def intersections(list1, list2, list3):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    s3 = set(list3)\n",
        "    set1 = s1.intersection(s2)\n",
        "    set1 =  (s2 & s3)\n",
        "    result = list(set1 & s3)\n",
        "    #intersections = list(set1)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hs5rGoW3VPo"
      },
      "outputs": [],
      "source": [
        "influential_nodes = intersections(central_nodes_degree, central_nodes_eigenvector, central_nodes_katz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0bOeegO-IDl"
      },
      "outputs": [],
      "source": [
        "_G = G.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHKZ3kJv6Hkk"
      },
      "outputs": [],
      "source": [
        "#_G = G.copy()\n",
        "for n, d in G.copy().nodes(data=True):\n",
        "  if(n not in influential_nodes):\n",
        "    G.remove_node(n)\n",
        "\n",
        "print(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9_XbtpS6Yqs"
      },
      "outputs": [],
      "source": [
        "#!pip3 install dwave_networkx.maximum_clique\n",
        "!git clone https://github.com/hvidberrrg/d-wave.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIfJDFQBIPEg"
      },
      "outputs": [],
      "source": [
        "!cd d-wave && pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtoOTl14Cx2_"
      },
      "outputs": [],
      "source": [
        "import dwave_networkx as dnx\n",
        "import neal\n",
        "solver = neal.SimulatedAnnealingSampler()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uoe67WsuOyQp"
      },
      "outputs": [],
      "source": [
        "from networkx.algorithms.connectivity.connectivity import average_node_connectivity\n",
        "\n",
        "recursed_graph = G.copy()\n",
        "_cliques = []\n",
        "def recursive_maximal_cliques(G, cliques):\n",
        "\n",
        "  if(len(G.nodes) == 0):\n",
        "    #print(cliques)\n",
        "    return cliques\n",
        "  else:\n",
        "    clique = dnx.maximum_clique(G, solver)\n",
        "    cliques.append(clique)\n",
        "    for n in clique:\n",
        "      if(n in G.nodes):\n",
        "        G.remove_node(n)\n",
        "    recursive_maximal_cliques(G, _cliques)\n",
        "    return cliques\n",
        "\n",
        "_cliques = recursive_maximal_cliques(recursed_graph, _cliques)\n",
        "print(_cliques)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU2LUtiwZda-"
      },
      "outputs": [],
      "source": [
        "def connectivity_metric_and_high_priority_nodes(G, cliques):\n",
        "  node_connectivities = []\n",
        "  high_priority_nodes = []\n",
        "  for idx, x in enumerate(cliques):\n",
        "    print(idx, x)\n",
        "    if(len(cliques[idx]) > 1):\n",
        "      induced_subgraph = nx.induced_subgraph(G, cliques[idx])\n",
        "      avg_node_connectivity = nx.average_node_connectivity(induced_subgraph)\n",
        "      node_connectivities.append(avg_node_connectivity)\n",
        "    else:\n",
        "      high_priority_nodes.append(x.copy().pop())\n",
        "  print(node_connectivities)\n",
        "  print(high_priority_nodes)\n",
        "  return node_connectivities, high_priority_nodes\n",
        "avg_clique_connectivitities, high_priority_nodes = connectivity_metric_and_high_priority_nodes(G.copy(), _cliques)\n",
        "total_avg_clique_connectivities = np.sum(avg_clique_connectivitities) / len(avg_clique_connectivitities)\n",
        "print(total_avg_clique_connectivities)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV3Oj-ylvL5R"
      },
      "outputs": [],
      "source": [
        "def influential_node_distances(G, influential_nodes):\n",
        "  distance_matrix = []\n",
        "  distance = []\n",
        "  for idx, x in enumerate(influential_nodes):\n",
        "    for idy, y in enumerate(influential_nodes):\n",
        "      try:\n",
        "        distance = nx.shortest_path_length(G, x, y)\n",
        "      except nx.NetworkXNoPath:\n",
        "          continue\n",
        "      if(distance is not None):\n",
        "        distance_matrix.append([idx, idy, distance])\n",
        "  print(distance_matrix)\n",
        "  return distance_matrix\n",
        "\n",
        "node_distances = influential_node_distances(G.copy(), influential_nodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byrxf_LvDR1y"
      },
      "outputs": [],
      "source": [
        "from tables.utils import list_logged_instances\n",
        "from random import randint\n",
        "intersection = set(influential_nodes).intersection(set(high_priority_nodes))\n",
        "#print(intersection)\n",
        "def get_indices(influential_nodes, intersection):\n",
        "  indices = []\n",
        "  for i in intersection:\n",
        "    indices.append(influential_nodes.index(i))\n",
        "  #print(indices)\n",
        "  return indices\n",
        "\n",
        "indices = get_indices(influential_nodes.copy(), intersection)\n",
        "\n",
        "print(indices)\n",
        "#t_node_distances = np.transpose(node_distances.copy())\n",
        "addressed_priority_distances = []\n",
        "priority_node_distances = []\n",
        "\n",
        "arr_list = sorted(node_distances, key=lambda x: x[2], reverse=False)\n",
        "list_of_lists = [sub_arr for sub_arr in arr_list]\n",
        "print(list_of_lists)\n",
        "#print(list_of_lists[:])\n",
        "\n",
        "#print(decr_sort_distances)\n",
        "#print(decr_sort_distances)\n",
        "#arr_sorted = np.sort(node_distances, axis=1)\n",
        "\n",
        "def select_elements(list_of_lists, indices, influential_nodes, average_clique_connectiveness):\n",
        "    transmission_line_possibilities = []\n",
        "\n",
        "    for sublist in list_of_lists:\n",
        "        if sublist[0] in indices and sublist[2] < 3:\n",
        "            transmission_line_possibilities.append(sublist)\n",
        "        if sublist[0] == sublist[1]:\n",
        "          for i in indices:\n",
        "            transmission_line_possibilities.append([sublist[0], i])\n",
        "\n",
        "\n",
        "\n",
        "    #transmission_line_redirect_list = sorted(transmission_line_possibilities, key=lambda x: x[2])\n",
        "    return transmission_line_possibilities\n",
        "\n",
        "transmission_line_redirect_list =  select_elements(list_of_lists, indices, influential_nodes, total_avg_clique_connectivities)\n",
        "print(transmission_line_redirect_list)\n",
        "len(transmission_line_redirect_list)\n",
        "\n",
        "#print(result)\n",
        "#sorted_dist = decr_sort_distances.copy()\n",
        "#print(sorted_dist)\n",
        "\n",
        "#for i in indices:\n",
        "  #priority_node_distances.append(np.where(sorted_dist[i]) != 0)\n",
        "#len(priority_node_distances)\n",
        "#print(priority_node_distances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSxvkD07-xmt"
      },
      "outputs": [],
      "source": [
        "def convert_elements_to_nodes(list_of_lists, influential_nodes):\n",
        "    converted_list = []\n",
        "    list_copy = list_of_lists.copy()\n",
        "    influential_nodes_copy = influential_nodes.copy()\n",
        "    for sublist in list_copy:\n",
        "        sublist[0] = influential_nodes_copy[sublist[0]]\n",
        "        sublist[1] = influential_nodes_copy[sublist[1]]\n",
        "        converted_list.append([sublist[0], sublist[1]])\n",
        "    return converted_list\n",
        "\n",
        "converted = convert_elements_to_nodes(transmission_line_redirect_list, influential_nodes)\n",
        "len(converted)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8SNEhpfDHfd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0PGVViKHzhU"
      },
      "outputs": [],
      "source": [
        "print(converted)\n",
        "#increment in greater random amounts of these edges being formed and see how the robustness of the netowrk changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9YRUCcfYgjB"
      },
      "outputs": [],
      "source": [
        "def add_edges(edges, G):\n",
        "    G2 = nx.Graph()\n",
        "    G2 = G.copy()\n",
        "    for sublist in edges:\n",
        "        G2.add_edge(sublist[0], sublist[1])\n",
        "    nx.write_gexf(G, 'MODIFIED_GRAPH.gexf')\n",
        "    return G2\n",
        "G2 = add_edges(converted, _G)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWzVgydHdQjy"
      },
      "outputs": [],
      "source": [
        "def BFS(G, init_node, distances_family):\n",
        "    queue = [init_node]\n",
        "    distances_family [init_node] = (0, init_node)\n",
        "    while queue:\n",
        "        node = queue.pop(0)\n",
        "        for neighbour in G.neighbors(node):\n",
        "            if distances_family[node][0] + 1 <= distances_family[neighbour][0]:\n",
        "                queue.append(neighbour)\n",
        "                distances_family[neighbour] = (distances_family[node][0] + 1, init_node)\n",
        "    return distances_family\n",
        "\n",
        "\n",
        "def calculate_distances(G, list_of_generators):\n",
        "    distances = [(100, -1)]*(G.number_of_nodes() + 1)\n",
        "\n",
        "    for node in list_of_generators:\n",
        "        distances = BFS(G, node, distances)\n",
        "\n",
        "    return distances\n",
        "\n",
        "distances_family_G1 = calculate_distances(_G, influential_nodes)\n",
        "distances_family_G2 = calculate_distances(G2, influential_nodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HaLRWQzvNbl"
      },
      "outputs": [],
      "source": [
        "def get_consumers(G, list_of_generators):\n",
        "    consumers = {}\n",
        "    distances_family = calculate_distances(G, list_of_generators)\n",
        "    for node in G.nodes:\n",
        "        family = distances_family[node][1]\n",
        "        if family not in consumers.keys():\n",
        "            consumers[family] = [node]\n",
        "        else:\n",
        "            consumers[family].append(node)\n",
        "    return consumers\n",
        "\n",
        "consumers_1 = get_consumer(_G, influential_nodes)\n",
        "consumers_2 = get_consumers(G2, influential_nodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joFj4Vgzvp8K"
      },
      "outputs": [],
      "source": [
        "def get_capacities(G, list_of_generators):\n",
        "    capacities = {}\n",
        "    consumers = get_consumers(G, list_of_generators)\n",
        "    for generator in consumers.keys():\n",
        "        capacities[generator] = len(consumers[generator])\n",
        "    return capacities\n",
        "\n",
        "def shutdown_evaluations(G, closed_gen, list_of_generators):\n",
        "    G2 = nx.Graph()\n",
        "    G2 = G.copy()\n",
        "    capacities_original = get_capacities(G2, list_of_generators)\n",
        "\n",
        "    removed_nodes = []\n",
        "    nodes_to_remove = [closed_gen]\n",
        "    while nodes_to_remove:\n",
        "        node_to_remove = nodes_to_remove.pop(0)\n",
        "        removed_nodes.append(node_to_remove)\n",
        "        list_of_generators.remove(node_to_remove)\n",
        "        new_capacities = get_capacities(G2, list_of_generators)\n",
        "        for node in list_of_generators:\n",
        "            if new_capacities[node] > 2*capacities_original[node]:\n",
        "                if node not in nodes_to_remove:\n",
        "                    nodes_to_remove.append(node)\n",
        "    return removed_nodes\n",
        "\n",
        "def simulate(G, list_of_generators):\n",
        "    for generator in list_of_generators:\n",
        "        aux_list = list_of_generators.copy()\n",
        "        affected_generators = shutdown_evaluations(G, generator, aux_list)\n",
        "        if(len(affected_generators)>0):\n",
        "            print(generator,len(affected_generators))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXQvt_F7GRLK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv9fe-bq7Lug"
      },
      "outputs": [],
      "source": [
        "#simulate(G2, influential_nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK0hovUqerOO"
      },
      "source": [
        "# **A total of 62 shutdowns for all nodes tested! all nodes not reported were 0. This is the modified graph with the suggested edge changes. **\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbcHRN_27IUq"
      },
      "outputs": [],
      "source": [
        "#simulate(_G, influential_nodes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1dVKdKnvbO2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow-1c4KJfmNB"
      },
      "source": [
        "# **A total of 169 shutdowns for all nodes tested! all nodes not reported were 0. This is the original graph without any of the suggested edge changes. **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrTdmIpKfsLL"
      },
      "source": [
        "# **That's a decrease of 107 simulated shutdowns! Or a 63% decrease in shutdowns!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXbvUuFaDqaB"
      },
      "outputs": [],
      "source": [
        "def add_random_edges(G, num_edges):\n",
        "    \"\"\"Adds a specified number of random edges to a graph.\"\"\"\n",
        "    possible_edges = []\n",
        "    for u in G.nodes():\n",
        "        for v in G.nodes():\n",
        "            if u != v and not G.has_edge(u, v):\n",
        "                possible_edges.append((u, v))\n",
        "\n",
        "    if num_edges > len(possible_edges):\n",
        "        raise ValueError(\"Cannot add more edges than possible.\")\n",
        "\n",
        "    random.shuffle(possible_edges)\n",
        "    for _ in range(num_edges):\n",
        "        u, v = possible_edges.pop()\n",
        "        G.add_edge(u, v)\n",
        "\n",
        "def simulate(G, list_of_generators, num_suggested_edges=0, num_random_edges=0):\n",
        "    print(\"Original Simulation:\")\n",
        "    for generator in list_of_generators:\n",
        "        aux_list = list_of_generators.copy()\n",
        "        affected_generators = shutdown_evaluations(G, generator, aux_list)\n",
        "        if len(affected_generators) > 0:\n",
        "            print(generator, len(affected_generators))\n",
        "\n",
        "    if num_suggested_edges > 0:\n",
        "        G_suggested = G.copy()\n",
        "        add_edges(converted, G_suggested)\n",
        "\n",
        "        print(\"Simulation with Suggested Edges:\")\n",
        "        for generator in list_of_generators:\n",
        "            aux_list = list_of_generators.copy()\n",
        "            affected_generators = shutdown_evaluations(G_suggested, generator, aux_list)\n",
        "            if len(affected_generators) > 0:\n",
        "                print(generator, len(affected_generators))\n",
        "\n",
        "    if num_random_edges > 0:\n",
        "        G_random = G.copy()\n",
        "        add_random_edges(G_random, num_random_edges)\n",
        "\n",
        "        print(\"Simulation with Random Edges:\")\n",
        "        for generator in list_of_generators:\n",
        "            aux_list = list_of_generators.copy()\n",
        "            affected_generators = shutdown_evaluations(G_random, generator, aux_list)\n",
        "            if len(affected_generators) > 0:\n",
        "                print(generator, len(affected_generators))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOE39HJtDKow"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
